{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SundeepPatil_Module2Assignment.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "_d4ZpaHz0xro",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Objective: Predict if an teacher essay gets approved or not\n",
        "\n",
        " The competition dataset contains information from teachers' project applications to DonorsChoose.org including teacher attributes, school attributes, and the project proposals including application essays. Your objective is to predict whether or not a DonorsChoose.org project proposal submitted by a teacher will be approved.\n",
        "\n",
        "File descriptions\n",
        "train.csv - the training set\n",
        "test.csv - the test set (we use just the training set and divide it into training and validation)\n"
      ]
    },
    {
      "metadata": {
        "id": "ECDsif4u6sui",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Get glove Embeddings from Stanford website"
      ]
    },
    {
      "metadata": {
        "id": "1FWZs6ahzI9u",
        "colab_type": "code",
        "outputId": "b196c609-ae6a-4033-8f56-dd724de2ee11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        }
      },
      "cell_type": "code",
      "source": [
        "!wget  http://nlp.stanford.edu/data/glove.6B.zip\n",
        "  \n",
        "!unzip glove.6B.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-03-17 09:19:09--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2019-03-17 09:19:09--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  22.1MB/s    in 33s     \n",
            "\n",
            "2019-03-17 09:19:42 (25.1 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tMmdCnlLz3Z2",
        "colab_type": "code",
        "outputId": "5b0438d7-cfa9-4953-ba03-2cd2bddd7f8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "glove.6B.100d.txt  sample_data\ttrain.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LzNshe71rpF2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import Dense, Input, GlobalMaxPooling1D, LSTM\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.initializers import Constant"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sx1VsFhiy1cs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BASE_DIR = ''\n",
        "GLOVE_DIR = os.path.join(BASE_DIR, '')\n",
        "TEXT_DATA_DIR = os.path.join(BASE_DIR, '.')\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 1000\n",
        "MAX_NUM_WORDS = 20000\n",
        "EMBEDDING_DIM = 100\n",
        "VALIDATION_SPLIT = 0.2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h-KgPerQ4OSD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Create a mapping  word to its respective embedding"
      ]
    },
    {
      "metadata": {
        "id": "6BppNXIPy4O7",
        "colab_type": "code",
        "outputId": "40b7b7e5-3cde-4ae4-8cb6-97ab70f4f2be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "print('Indexing word vectors.')\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')) as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Indexing word vectors.\n",
            "Found 182431 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "axxE-4bF4aw6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load the text samples and process their dataset.We take only the train data here and split it as 80/20 train and validation set for effective generalization"
      ]
    },
    {
      "metadata": {
        "id": "s68WDOsY-teX",
        "colab_type": "code",
        "outputId": "0ecd89e8-a576-42a7-dda7-62a2fadcede1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv('./train.csv',encoding = 'utf8' ,  engine='python', error_bad_lines=False)\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Skipping line 7391: unexpected end of data\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "BiPXeFnTBH_6",
        "colab_type": "code",
        "outputId": "102a5787-9f89-446d-ae97-ff6a0be566ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "cell_type": "code",
      "source": [
        "data.columns"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['id', 'teacher_id', 'teacher_prefix', 'school_state',\n",
              "       'project_submitted_datetime', 'project_grade_category',\n",
              "       'project_subject_categories', 'project_subject_subcategories',\n",
              "       'project_title', 'project_essay_1', 'project_essay_2',\n",
              "       'project_essay_3', 'project_essay_4', 'project_resource_summary',\n",
              "       'teacher_number_of_previously_posted_projects', 'project_is_approved'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "metadata": {
        "id": "JmSXrNc-B3Lm",
        "colab_type": "code",
        "outputId": "cefcc2bb-4aac-46f0-be66-59d90e061e8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "cell_type": "code",
      "source": [
        "labels = data['project_is_approved'].tolist()\n",
        "texts = data['project_essay_1'].tolist()\n",
        "print('Found %s texts.' % len(texts))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 7389 texts.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NdAB-SXW4ht2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Tokenize the words "
      ]
    },
    {
      "metadata": {
        "id": "iv1Ve1Cz1pLf",
        "colab_type": "code",
        "outputId": "4ea709d3-81a6-4309-ddf7-d39031a2a196",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "cell_type": "code",
      "source": [
        "# finally, vectorize the text samples into a 2D integer tensor\n",
        "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 13527 unique tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9VYcZHog4nnY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Pad the sequences so that they are all the same"
      ]
    },
    {
      "metadata": {
        "id": "p-qICoQM1qDD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D1ljxiJf4uUU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## check data and label"
      ]
    },
    {
      "metadata": {
        "id": "Kna2JZy51ttS",
        "colab_type": "code",
        "outputId": "a25f3c5b-2b82-48bf-c063-e80eae110055",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "set(labels)\n",
        "labels = to_categorical(np.asarray(labels))\n",
        "print('Shape of data tensor:', data.shape)\n",
        "print('Shape of label tensor:', labels.shape)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of data tensor: (7389, 1000)\n",
            "Shape of label tensor: (7389, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BhJRmbFL09vT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "ab398633-117a-45f8-cc05-527ac899caf3"
      },
      "cell_type": "code",
      "source": [
        "labels"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       ...,\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "metadata": {
        "id": "KgGRrtvl7sl4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### split the data into a training set and a validation set\n"
      ]
    },
    {
      "metadata": {
        "id": "8cX0G3xW1xI4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "indices = np.arange(data.shape[0])\n",
        "indices = np.random.shuffle(indices)\n",
        "data = data[list(indices)]\n",
        "#labels = labels[indices]\n",
        "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GhX7KWlT4yZj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Make a train test split"
      ]
    },
    {
      "metadata": {
        "id": "yUqKkvC112xs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_train = data[:-num_validation_samples]\n",
        "y_train = labels[:-num_validation_samples]\n",
        "x_val = data[-num_validation_samples:]\n",
        "y_val = labels[-num_validation_samples:]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4IGlkUBt42KV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Embedding layer set to non trainable mode"
      ]
    },
    {
      "metadata": {
        "id": "pTWOy24G17-E",
        "colab_type": "code",
        "outputId": "0e6201fc-c255-42e3-d11c-2513215ffd00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "cell_type": "code",
      "source": [
        "print('Preparing embedding matrix.')\n",
        "\n",
        "# prepare embedding matrix\n",
        "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
        "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    if i > MAX_NUM_WORDS:\n",
        "        continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# load pre-trained word embeddings into an Embedding layer\n",
        "# note that we set trainable = False so as to keep the embeddings fixed\n",
        "embedding_layer = Embedding(num_words,\n",
        "                            EMBEDDING_DIM,\n",
        "                            embeddings_initializer=Constant(embedding_matrix),\n",
        "                            input_length=MAX_SEQUENCE_LENGTH,\n",
        "                            trainable=False)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preparing embedding matrix.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3UEgt6_T48WC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Define the model and compile"
      ]
    },
    {
      "metadata": {
        "id": "qSFZQOPj2AMO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "611ae1f7-a741-478c-f9af-d9a6e6a2bafc"
      },
      "cell_type": "code",
      "source": [
        "# train a 1D convnet with global maxpooling\n",
        "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "\n",
        "embedded_sequences = embedding_layer(sequence_input)\n",
        "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
        "x = MaxPooling1D(5)(x)\n",
        "x = Conv1D(128, 5, activation='relu')(x)\n",
        "x = MaxPooling1D(5)(x)\n",
        "x = Conv1D(128, 5, activation='relu')(x)\n",
        "x = GlobalMaxPooling1D()(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "preds = Dense(2, activation='softmax')(x)\n"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CzadLbP78UWa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = Model(sequence_input, preds)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['acc'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yZbcLM90-_-e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "7cb73b49-f5b9-4501-9c1b-074bce99260f"
      },
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   0,    0,    0, ...,    1,  373,  167],\n",
              "       [   0,    0,    0, ...,   11, 1204,   38],\n",
              "       [   0,    0,    0, ...,    8,  152,   49],\n",
              "       ...,\n",
              "       [   0,    0,    0, ..., 8402,    3,   85],\n",
              "       [   0,    0,    0, ...,   38,   29,  289],\n",
              "       [   0,    0,    0, ...,   53,   23,  202]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "metadata": {
        "id": "n8onZbUY80Wo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "class_weight ={0: 5.,\n",
        "               1: 1.0}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W8bFHxS98FEt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Fit a model to train and validation set"
      ]
    },
    {
      "metadata": {
        "id": "U2Oc1YPp2Erl",
        "colab_type": "code",
        "outputId": "bea0042d-033a-407b-8b8e-88ad8a4f8b71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        }
      },
      "cell_type": "code",
      "source": [
        "model.fit(x_train, y_train,\n",
        "          batch_size=128,\n",
        "          epochs=10,\n",
        "          class_weight = class_weight,\n",
        "          validation_data=(x_val, y_val))"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 5912 samples, validate on 1477 samples\n",
            "Epoch 1/10\n",
            "5912/5912 [==============================] - 3s 447us/sample - loss: 0.0684 - acc: 0.9805 - val_loss: 1.2082 - val_acc: 0.8416\n",
            "Epoch 2/10\n",
            "5912/5912 [==============================] - 2s 393us/sample - loss: 0.0631 - acc: 0.9785 - val_loss: 1.2632 - val_acc: 0.7915\n",
            "Epoch 3/10\n",
            "5912/5912 [==============================] - 2s 394us/sample - loss: 0.0979 - acc: 0.9660 - val_loss: 1.0415 - val_acc: 0.7698\n",
            "Epoch 4/10\n",
            "5912/5912 [==============================] - 2s 391us/sample - loss: 0.0765 - acc: 0.9768 - val_loss: 1.1867 - val_acc: 0.8043\n",
            "Epoch 5/10\n",
            "5912/5912 [==============================] - 2s 392us/sample - loss: 0.0919 - acc: 0.9745 - val_loss: 1.1236 - val_acc: 0.7982\n",
            "Epoch 6/10\n",
            "5912/5912 [==============================] - 2s 388us/sample - loss: 0.0452 - acc: 0.9853 - val_loss: 2.1782 - val_acc: 0.4719\n",
            "Epoch 7/10\n",
            "5912/5912 [==============================] - 2s 390us/sample - loss: 0.1198 - acc: 0.9706 - val_loss: 1.0687 - val_acc: 0.6899\n",
            "Epoch 8/10\n",
            "5912/5912 [==============================] - 2s 393us/sample - loss: 0.0482 - acc: 0.9860 - val_loss: 1.1362 - val_acc: 0.8165\n",
            "Epoch 9/10\n",
            "5912/5912 [==============================] - 2s 390us/sample - loss: 0.0563 - acc: 0.9799 - val_loss: 1.2570 - val_acc: 0.7921\n",
            "Epoch 10/10\n",
            "5912/5912 [==============================] - 2s 388us/sample - loss: 0.0572 - acc: 0.9819 - val_loss: 1.3295 - val_acc: 0.8138\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fcb9e00ff60>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "metadata": {
        "id": "ovc9qEQ36vIh",
        "colab_type": "code",
        "outputId": "9465e056-2171-4a72-b216-675cc94685cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        }
      },
      "cell_type": "code",
      "source": [
        "model.fit(x_train, y_train,\n",
        "          batch_size=128,\n",
        "          epochs=10,\n",
        "          validation_data=(x_val, y_val))"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 5912 samples, validate on 1477 samples\n",
            "Epoch 1/10\n",
            "5912/5912 [==============================] - 3s 445us/sample - loss: 0.0379 - acc: 0.9893 - val_loss: 1.2014 - val_acc: 0.7833\n",
            "Epoch 2/10\n",
            "5912/5912 [==============================] - 2s 390us/sample - loss: 0.0390 - acc: 0.9868 - val_loss: 1.2705 - val_acc: 0.6919\n",
            "Epoch 3/10\n",
            "5912/5912 [==============================] - 2s 391us/sample - loss: 0.0279 - acc: 0.9892 - val_loss: 1.5909 - val_acc: 0.7989\n",
            "Epoch 4/10\n",
            "5912/5912 [==============================] - 2s 392us/sample - loss: 0.0349 - acc: 0.9883 - val_loss: 1.6347 - val_acc: 0.8057\n",
            "Epoch 5/10\n",
            "5912/5912 [==============================] - 2s 394us/sample - loss: 0.0283 - acc: 0.9900 - val_loss: 1.4807 - val_acc: 0.7888\n",
            "Epoch 6/10\n",
            "5912/5912 [==============================] - 2s 393us/sample - loss: 0.0391 - acc: 0.9897 - val_loss: 1.6289 - val_acc: 0.8179\n",
            "Epoch 7/10\n",
            "5912/5912 [==============================] - 2s 393us/sample - loss: 0.0338 - acc: 0.9860 - val_loss: 1.4397 - val_acc: 0.8172\n",
            "Epoch 8/10\n",
            "5912/5912 [==============================] - 2s 392us/sample - loss: 0.0249 - acc: 0.9890 - val_loss: 1.7755 - val_acc: 0.8009\n",
            "Epoch 9/10\n",
            "5912/5912 [==============================] - 2s 386us/sample - loss: 0.0437 - acc: 0.9846 - val_loss: 1.8166 - val_acc: 0.8091\n",
            "Epoch 10/10\n",
            "5912/5912 [==============================] - 2s 387us/sample - loss: 0.0785 - acc: 0.9790 - val_loss: 1.3865 - val_acc: 0.8104\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fcb9e00f5c0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "metadata": {
        "id": "s3n0XrQjg0L8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
